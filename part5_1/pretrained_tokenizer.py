# -*- coding: utf-8 -*-
"""pretrained_tokenizer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qOP82ocAnBH6w9jVEGZTBJ2de2p7E9g5

- 잘 만들어진 token vocabulary를 이용해 문장을 어떻게 tokenization하는지 간단하게 살펴보자.
- BPE Tokenizer는 구글이 제공하는 sentencepiece를 많이 사용
"""

pip install sentencepiece

from google.colab import drive
drive.mount('/content/drive')

import sentencepiece as spm

s = spm.SentencePieceProcessor(model_file = '/content/drive/MyDrive/Colab Notebooks/test_model.model')

for _ in range(10):
  print(s. encode('This is a test', out_type=str, enable_sampling=True, alpha=0.1, nbest_size=-1))

s.decode([284, 47, 11, 4, 15, 400])

"""- 하지만 여기서 sentencepiece의 사용법을 알아보기보다는 나중에 모델 BERT에서 사용할 tokenizer 사용법을 알아보자.
- 이를 위해서는 transformers라는 라이브러리를 설치해야 함.
- 설치 후 다음 코드를 이용해 이미 학습이 완료된 tokenizer와 vocabulary를 쉽게 가져올 수 있음.
"""

pip install transformers

from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') # 'bert-base-uncased'라는 이름의 이미 학습된 모델
# 해당 모델을 사용하려면 모델 학습을 위해 사용했던 tokenizer도 일치시켜야 함.
# 그래서 연구자가 미리 학습해둔 tokenizer를 가져와 사용함.

print(len(tokenizer.vocab)) # vocab : tokenizer의 크기 확인 가능

"""- 그렇다면 해당 tokenizer는 문장을 어떻게 tokenization하는지 알아보자."""

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
sentence = 'My dog is cute. He likes playing'
print(tokenizer.tokenize(sentence))

"""- split()과 다르지 않을 결과를 보임."""

# 다양한 언어를 담고 있는 다른 데이터에서 학습한 모델인 bert-base-multilingual-uncased의 tokenizer를 가져와 같은 문장을 tokenization하면 다름
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')
print(len(tokenizer.vocab))
print('\n')
print(tokenizer.tokenize(sentence))

"""- 'cute'가 'cut'와 '##e'로 나뉨.
- 이렇게 학습한 데이터에 따라 tokenizer가 달라지는 것을 확인할 수 있다.
- (참고) '##e'는 앞에 띄어쓰기가 아닌 바로 이어지는 token을 의미
"""

sentence = '나는 책상 위에 사과를 먹었다. 알고 보니 그 사과는 Jason 것이었다. 그래서 Jason에게 사과를 했다'
print(tokenizer.tokenize(sentence))

"""- 위 multilingual model에는 한국어 데이터도 포함돼 있어서 한국어에 대한 tokenization도 해볼 수 있다. 
- 다양한 언어에 대한 tokenizer이다 보니 매우 제한적인 vocabulary size 제약하에서 최적의 token을 구해야 하기 때문에 token이 짧아질 수밖에 없어 위와 같이 많이 잘리는 듯한 결과를 보여줌.
"""