{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "falling-positive",
   "metadata": {},
   "source": [
    "## 4. CNN Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-clause",
   "metadata": {},
   "source": [
    "- __*ImageNet*__\n",
    "- 이미지 분류 모델을 측정하기 위한 데이터로 가장 많이 사용하는 데이터셋\n",
    "- 학습 데이터만 총 138GB로, 총 2만 개 이상의 클래스와 약 1400만 장의 이미지로 구성\n",
    "- 이 데이터를 이용해 이미지로 __분류__하는 모델 대회 = ImageNet Large Scale Visual Recognition Challenge (ILSVRC)  \n",
    "- 2015년을 기점으로 인간의 성능을 뛰어넘었으며, 네트워크의 깊이는 152층에 이름.\n",
    "\n",
    "1. __<span style=\"color:blue\">LeNet</span>__ \n",
    "- Yann LeCun 교수가 제안한 최초의 CNN 모델\n",
    "- 1990년대 당시 컴퓨팅 문제로 인해 비교적 단순한 구조.\n",
    "- 32 * 32 input과 conv layer 2개, pooling layer 2개, FC layer 3개  \n",
    "    => 이 구조는 __가장 기본적인 CNN 구조__로 사용됨.\n",
    "    \n",
    "2. __<span style=\"color:blue\">AlexNet</span>__\n",
    "- 2012 ILSVRC 대회 우승 모델\n",
    "- 구조는 LeNet과 크게 다르지 않음\n",
    "- 224 * 224 크기의 RGB 3 channel image를 input으로 사용\n",
    "- activation 함수로는 ReLU 사용\n",
    "- dropout과 data augmentation 등을 적용 (처음으로 augmentation을 heavy하게 적용)\n",
    "\n",
    "3.  __<span style=\"color:blue\">VGGNet</span>__\n",
    "- 2014 ILSVRC 대회에서 2위를 차지한 모델\n",
    "- AlexNet과 같은 이전의 모델과 달리, __3 * 3 conv layer를 깊게 중첩__한다는 것이 VGG의 큰 특징\n",
    "- layer의 깊이에 따라 VGG16, VGG19 등으로 불리고 있다.\n",
    "- 이미지 데이터에 가장 많이 사용하는 preprocessing, augmentation 방법이 VGGNet이 했던 방식\n",
    "\n",
    "4.  __<span style=\"color:blue\">GoogleNet</span>__ = inception model\n",
    "- 2014 ILSVRC 대회에서 1위한 모델\n",
    "- 구글에서 제안한 모델로, Google + LeNet을 합친 말\n",
    "- 매우 복잡한 형태를 지니고 있음.\n",
    "- __inception module__이라는 개념을 CNN에 도입\n",
    "- 기존의 CNN 구조는 conv 다음 pooling layer를 거치는 것이 일반적.  \n",
    "    => inception module은 한 layer 내에서 서로 다른 연산을 거친 후 feature map을 다시 합치는 방식\n",
    "- 이러한 방식을 사용하면 __한 feature map에서 여러 convolution을 적용할 수 있기 때문에 작은 규모의 feature, 비교적 큰 규모의 feature를 한 번에 학습할 수 있다는 장점__이 있음.\n",
    "- GoogleNet은 총 9개의 inception module로 구성돼 있음.\n",
    "- 또한, 마지막 FC layer에서 GAP(Global Average Pooling)으로 대체해 파라미터의 수를 크게 줄이는 효과를 얻음. \n",
    "- __GAP__  \n",
    ": 마지막 feature map에 대해 각각의 값을 평균내 연결해주는 것. FC layer 대비 학습해야 할 파라미터의 수를 크게 줄일 수 있음.\n",
    "\n",
    "5.  __<span style=\"color:blue\">ResNet</span>__ = Residual Network\n",
    "- 마이크로소프트에서 제안한 모델\n",
    "- 2015 ILSVRC 대회에서 1위를 차지한 모델\n",
    "- 지금까지도 이미지 분류의 기본 모델로 널리 쓰이고 있음. 많은 딥러닝 연구 논문에서도 기본 딥러닝 프레임으로 활용되고 있음.\n",
    "- __residual block이라는 개념__을 도입  \n",
    ": 이전 layer의 feature map을 다음 layer의 feature map에 더해주는 개념. = __skip connection__\n",
    "- 즉, __이전 layer와 다음 layer에 skip connection을 적용하는 모델__\n",
    "- 네트워크가 깊어짐에 따라 앞 단의 layer에 대한 정보는 뒤의 layer에서는 희석될 수밖에 없다.  \n",
    "    => 이러한 단점을 보완하기 위해 __이전의 정보를 뒤에서도 함께 활용__하는 개념"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bearing-fifteen",
   "metadata": {},
   "source": [
    "## 5. Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ruled-stable",
   "metadata": {},
   "source": [
    "- 강아지와 고양이를 구분하는 딥러닝 모델을 구축하고자 한다.\n",
    "- 수많은 데이터가 필요하지만 현재 보유하고 있는 강아지, 고양이 사진은 약 100여 장에 불과하다.\n",
    "- 고양이와 강아지가 지니고 있는 일반적인 feature를 충분히 잘 학습하기 어렵기 때문에 모델의 충분한 학습 어려움.  \n",
    "=> 이런 상황에 주로 __전이 학습(transfer learning)__ 사용\n",
    "\n",
    "\n",
    "- __<span style=\"color:blue\">전이 학습(transfer learning)?</span>__  \n",
    ": (보유한 데이터가 많이 않을 때)__데이터를 미리 학습해 놓은 딥러닝 모델(pre-trained model) 가져와 재학습(fine-tuning)시키는 방법__  \n",
    ": pre-trained model이 수많은 데이터에 대해 학습시켜 놓았기 때문에 그들의 feature를 활용하는 것.\n",
    "- pre-trained model은 기본적으로 파이토치에서 다운로드 해 사용 가능\n",
    "- pre-trained model을 로드한 후 FC layer 앞단 네트워크의 weight를 가져오고 FC layer를 디자인함. (이때, FC layer도 그대로 사용하고 output layer만 디자인 하기도 함.)\n",
    "- 그리고 우리가 보유한 데이터를 input으로 해 학습을 진행하는데, 일반적으로 pre-trained model의 FC layer 이전의 weight는 학습시키지 않음.  \n",
    "    => 이때에는 __weight를 freezing한다__고 표현.  \n",
    "    => 이 과정을 __fine-tuning__이라고 함.  \n",
    "    \n",
    "    \n",
    "- transfer learning은 __initialization의 개념__으로 바라볼 수도 있다.  \n",
    "    => transfer learning은 내가 학습하고자 하는 모델의 초기 weight에 pre-trained model의 weight를 사용하는 것과 같기 때문\n",
    "- 네트워크의 weight initialization 기법은 네트워크의 높은 성능과 빠른 수렴을 위해 연구되고 있다.\n",
    "- 일반적으로 데이터가 많지 않은 상황에서 보유하고 있는 데이터만으로 학습시켰을 때보다 transfer learning을 시켰을 때 모델 성능이 높다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
